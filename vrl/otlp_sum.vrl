# @schema sum
# @description OTLP sum metrics flattened for Cloudflare Streams
#
# _signal: string, required, "Signal type for deterministic sorting"
# timestamp: timestamp, required, "Observation time in seconds"
# start_timestamp: int64, "Start time in seconds"
# metric_name: string, required, "Metric name"
# metric_description: string, "Metric description"
# metric_unit: string, "Unit (e.g., ms, bytes, 1)"
# value: float64, required, "Metric value"
# service_name: string, required, "Service name from resource"
# service_namespace: string
# service_instance_id: string
# resource_attributes: json, "Resource attributes blob"
# scope_name: string, "Instrumentation scope name"
# scope_version: string, "Instrumentation scope version"
# scope_attributes: json, "Scope attributes blob"
# metric_attributes: json, "Data point attributes blob"
# flags: int32, "Data point flags"
# exemplars_json: json, "Exemplars with trace context"
# aggregation_temporality: int32, required, "1=delta, 2=cumulative"
# is_monotonic: bool, required, "True for counters"
# @end

# vrl/otlp_sum.vrl
# OTLP sum metrics -> flat metric event
# Uses custom WASM-compatible functions (not VRL stdlib)
# Note: Records are pre-partitioned by _metric_type in Rust before reaching VRL

# Convert nanoseconds to milliseconds (must be integer for schema)
.timestamp = to_int(.time_unix_nano) ?? 0
.timestamp = floor(.timestamp / 1000000) ?? 0

.start_timestamp = to_int(.start_time_unix_nano) ?? 0
.start_timestamp = floor(.start_timestamp / 1000000) ?? 0

# Extract metric name
.metric_name, err = to_string(.metric_name)
if err != null {
    .metric_name = ""
}

# Extract description and unit
.metric_description, err = to_string(.metric_description)
if err != null {
    .metric_description = ""
}

.metric_unit, err = to_string(.metric_unit)
if err != null {
    .metric_unit = ""
}

# Value is already set by decoder

# Extract service info from resource attributes
# Use explicit null check since get() returns null for missing keys
.service_name_raw, err = get(.resource.attributes, ["service.name"])
if err != null || .service_name_raw == null {
    .service_name = "unknown"
} else {
    .service_name = .service_name_raw
}
.service_name_raw = null

.service_namespace = get(.resource.attributes, ["service.namespace"]) ?? null
.service_instance_id = get(.resource.attributes, ["service.instance.id"]) ?? null

# Encode attribute blobs as JSON
if !is_empty(.resource.attributes) {
    .resource_attributes = encode_json(.resource.attributes)
} else {
    .resource_attributes = null
}

.scope_name_str, err = to_string(.scope.name)
if err == null && .scope_name_str != "" {
    .scope_name = .scope_name_str
} else {
    .scope_name = null
}

.scope_version_str, err = to_string(.scope.version)
if err == null && .scope_version_str != "" {
    .scope_version = .scope_version_str
} else {
    .scope_version = null
}

if !is_empty(.scope.attributes) {
    .scope_attributes = encode_json(.scope.attributes)
} else {
    .scope_attributes = null
}

if !is_empty(.attributes) {
    .metric_attributes = encode_json(.attributes)
} else {
    .metric_attributes = null
}

# Encode exemplars as JSON
if !is_empty(.exemplars) {
    .exemplars_json = encode_json(.exemplars)
} else {
    .exemplars_json = null
}

# Flags
if .flags == null {
    .flags = 0
}

# Sum-specific fields
if .aggregation_temporality == null {
    .aggregation_temporality = 0
}

if .is_monotonic == null {
    .is_monotonic = false
}

# Clean up temporary and nested structures (set to null, filtered during serialization)
._metric_type = null
.scope_name_str = null
.scope_version_str = null
.time_unix_nano = null
.start_time_unix_nano = null
.resource = null
.scope = null
.attributes = null
.exemplars = null

# Set signal type and routing table
._signal = "sum"
._table = "sum"
