#!/bin/bash
#
# Unified AWS deployment script for otlp2pipeline
#
# This script handles the complete deployment lifecycle:
# - S3 Tables + Lake Formation setup
# - CloudFormation stack (table bucket, namespace, tables, IAM role, logging)
# - LakeFormation permissions grant
# - Firehose streams via API (AppendOnly mode for unlimited throughput)
#
# Prerequisites:
# - AWS CLI configured with credentials
# - Caller must have IAM and LakeFormation admin permissions
# - CloudFormation template generated by: otlp2pipeline aws create --output template.yaml
#
# Usage:
#   ./scripts/aws-deploy.sh <template> --env <env> [--region <region>] [--namespace <ns>]
#   ./scripts/aws-deploy.sh status --env <env> [--region <region>]
#   ./scripts/aws-deploy.sh destroy --env <env> [--region <region>] [--force]
#
# Examples:
#   ./scripts/aws-deploy.sh template.yaml --env prod
#   ./scripts/aws-deploy.sh template.yaml --env prod --region us-west-2
#   ./scripts/aws-deploy.sh status --env prod
#   ./scripts/aws-deploy.sh destroy --env prod --force
#
# Naming:
#   --env prod  →  stack: otlp2pipeline-prod, bucket: otlp2pipeline-prod
#

set -e

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[0;33m'
NC='\033[0m' # No Color

# Symbols
CHECK="${GREEN}✓${NC}"
CROSS="${RED}✗${NC}"
ARROW="${YELLOW}→${NC}"

# Check prerequisites
if ! command -v aws &> /dev/null; then
    echo -e "${CROSS} AWS CLI is not installed."
    echo "    Install from: https://aws.amazon.com/cli/"
    exit 1
fi

# Defaults
REGION="us-east-1"
NAMESPACE="default"
ROLE_NAME="S3TablesRoleForLakeFormation"
FORCE=false
ENV=""

# Parse arguments
COMMAND=""
TEMPLATE=""
POSITIONAL=()

while [[ $# -gt 0 ]]; do
    case $1 in
        status|destroy)
            COMMAND="$1"
            shift
            ;;
        --env)
            ENV="$2"
            shift 2
            ;;
        --region)
            REGION="$2"
            shift 2
            ;;
        --namespace)
            NAMESPACE="$2"
            shift 2
            ;;
        --force)
            FORCE=true
            shift
            ;;
        --help|-h)
            echo "Usage:"
            echo "  $0 <template.yaml> --env <env> [--region <region>] [--namespace <ns>]"
            echo "  $0 status --env <env> [--region <region>]"
            echo "  $0 destroy --env <env> [--region <region>] [--force]"
            echo ""
            echo "Commands:"
            echo "  (default)  Deploy all infrastructure (idempotent)"
            echo "  status     Check current deployment status"
            echo "  destroy    Tear down all infrastructure"
            echo ""
            echo "Options:"
            echo "  --env        Environment name (required) - derives stack and bucket names"
            echo "  --region     AWS region (default: us-east-1)"
            echo "  --namespace  S3 Tables namespace (default: default)"
            echo "  --force      Skip confirmation for destroy"
            echo ""
            echo "Naming convention:"
            echo "  --env prod  →  stack: otlp2pipeline-prod, bucket: otlp2pipeline-prod"
            exit 0
            ;;
        -*)
            echo "Unknown option: $1"
            exit 1
            ;;
        *)
            POSITIONAL+=("$1")
            shift
            ;;
    esac
done

# Handle positional arguments
if [ ${#POSITIONAL[@]} -gt 0 ]; then
    if [ -z "$COMMAND" ]; then
        TEMPLATE="${POSITIONAL[0]}"
    fi
fi

# Validate required arguments
if [ -z "$ENV" ]; then
    echo "Error: --env is required"
    exit 1
fi

# Normalize env name (strip otlp2pipeline- prefix if present)
ENV="${ENV#otlp2pipeline-}"

# Derive stack and bucket names from env
STACK="otlp2pipeline-${ENV}"
BUCKET="otlp2pipeline-${ENV}"

# Validate name lengths early (S3 bucket names max 63 chars)
# Error bucket format: ${STACK}-errors-${ACCOUNT_ID}-${REGION}
# With 12-char account ID and up to 14-char region, stack must be ≤28 chars
validate_name_lengths() {
    local stack_len=${#STACK}
    # Error bucket: stack + "-errors-" (8) + account_id (12) + "-" (1) + region (up to 14) = stack + 35
    local max_stack_len=$((63 - 8 - 12 - 1 - ${#REGION}))

    if [ "$stack_len" -gt "$max_stack_len" ]; then
        local error_bucket_len=$((stack_len + 8 + 12 + 1 + ${#REGION}))
        echo -e "${CROSS} Stack name '${STACK}' is too long (${stack_len} chars)"
        echo "    Error bucket would be ${error_bucket_len} chars (max 63)"
        echo "    Max stack name length for region ${REGION}: ${max_stack_len} chars"
        echo ""
        echo "    Use a shorter --env name (current: ${ENV})"
        exit 1
    fi
}

# Get AWS account info
get_account_info() {
    ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text 2>/dev/null) || {
        echo -e "${CROSS} Failed to get AWS account info. Check your credentials."
        exit 1
    }
    CALLER_ARN=$(aws sts get-caller-identity --query Arn --output text 2>/dev/null) || {
        echo -e "${CROSS} Failed to get caller ARN. Check your credentials."
        exit 1
    }
    if [ -z "$CALLER_ARN" ]; then
        echo -e "${CROSS} Caller ARN is empty. Check your AWS credentials."
        exit 1
    fi
}

# Run an idempotent AWS command, accepting specific expected errors
# Usage: run_idempotent "description" "expected_error_pattern" aws command args...
# If expected_error_pattern is empty, any error will cause failure
run_idempotent() {
    local desc="$1"
    local expected_error="$2"
    shift 2

    local output
    if output=$("$@" 2>&1); then
        return 0
    else
        local exit_code=$?
        if [ -n "$expected_error" ] && echo "$output" | grep -q "$expected_error"; then
            # Expected error (e.g., AlreadyExistsException), treat as success
            return 0
        else
            echo -e "    ${CROSS} ${desc}: $output"
            exit $exit_code
        fi
    fi
}

# Policy documents for S3 Tables role
TRUST_POLICY='{
    "Version": "2012-10-17",
    "Statement": [{
        "Effect": "Allow",
        "Principal": {"Service": "lakeformation.amazonaws.com"},
        "Action": ["sts:AssumeRole", "sts:SetSourceIdentity", "sts:SetContext"]
    }]
}'

S3_TABLES_POLICY='{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "LakeFormationPermissionsForS3ListTableBucket",
            "Effect": "Allow",
            "Action": ["s3tables:ListTableBuckets"],
            "Resource": ["*"]
        },
        {
            "Sid": "LakeFormationDataAccessPermissionsForS3TableBucket",
            "Effect": "Allow",
            "Action": [
                "s3tables:CreateTableBucket",
                "s3tables:GetTableBucket",
                "s3tables:CreateNamespace",
                "s3tables:GetNamespace",
                "s3tables:ListNamespaces",
                "s3tables:DeleteNamespace",
                "s3tables:DeleteTableBucket",
                "s3tables:CreateTable",
                "s3tables:DeleteTable",
                "s3tables:GetTable",
                "s3tables:ListTables",
                "s3tables:RenameTable",
                "s3tables:UpdateTableMetadataLocation",
                "s3tables:GetTableMetadataLocation",
                "s3tables:GetTableData",
                "s3tables:PutTableData"
            ],
            "Resource": ["*"]
        }
    ]
}'

# ============================================================================
# Status checks
# ============================================================================

check_iam_role() {
    aws iam get-role --role-name "${ROLE_NAME}" >/dev/null 2>&1
}

check_lakeformation_admin() {
    local admins
    admins=$(aws lakeformation get-data-lake-settings --region "${REGION}" \
        --query 'DataLakeSettings.DataLakeAdmins[*].DataLakePrincipalIdentifier' \
        --output text 2>/dev/null) || return 1
    echo "$admins" | grep -q "${CALLER_ARN}"
}

check_lakeformation_resource() {
    local resource_arn="arn:aws:s3tables:${REGION}:${ACCOUNT_ID}:bucket/*"
    aws lakeformation describe-resource --resource-arn "${resource_arn}" --region "${REGION}" >/dev/null 2>&1
}

check_glue_catalog() {
    aws glue get-catalog --catalog-id "s3tablescatalog" --region "${REGION}" >/dev/null 2>&1
}

check_stack_exists() {
    aws cloudformation describe-stacks --stack-name "${STACK}" --region "${REGION}" >/dev/null 2>&1
}

get_stack_status() {
    aws cloudformation describe-stacks --stack-name "${STACK}" --region "${REGION}" \
        --query 'Stacks[0].StackStatus' --output text 2>/dev/null
}

get_firehose_role_arn() {
    aws cloudformation describe-stacks --stack-name "${STACK}" --region "${REGION}" \
        --query 'Stacks[0].Outputs[?OutputKey==`FirehoseRoleARN`].OutputValue' \
        --output text 2>/dev/null
}

check_lakeformation_db_permission() {
    local role_arn="$1"
    aws lakeformation list-permissions \
        --principal "{\"DataLakePrincipalIdentifier\":\"${role_arn}\"}" \
        --resource "{\"Database\":{\"CatalogId\":\"${ACCOUNT_ID}:s3tablescatalog/${BUCKET}\",\"Name\":\"${NAMESPACE}\"}}" \
        --region "${REGION}" \
        --query 'PrincipalResourcePermissions[0]' \
        --output text 2>/dev/null | grep -q DESCRIBE
}

check_lakeformation_table_permission() {
    local role_arn="$1"
    local table_name="$2"
    aws lakeformation list-permissions \
        --principal "{\"DataLakePrincipalIdentifier\":\"${role_arn}\"}" \
        --resource "{\"Table\":{\"CatalogId\":\"${ACCOUNT_ID}:s3tablescatalog/${BUCKET}\",\"DatabaseName\":\"${NAMESPACE}\",\"Name\":\"${table_name}\"}}" \
        --region "${REGION}" \
        --query 'PrincipalResourcePermissions[0]' \
        --output text 2>/dev/null | grep -q ALL
}

# ============================================================================
# Status command
# ============================================================================

cmd_status() {
    echo "Checking deployment status..."
    echo ""
    get_account_info
    echo "Account: ${ACCOUNT_ID}"
    echo "Region:  ${REGION}"
    echo "Stack:   ${STACK}"
    echo "Bucket:  ${BUCKET}"
    echo ""

    echo "S3 Tables Setup:"
    if check_iam_role; then
        echo -e "  ${CHECK} IAM Role: ${ROLE_NAME}"
    else
        echo -e "  ${CROSS} IAM Role: ${ROLE_NAME} (not found)"
    fi

    if check_lakeformation_admin; then
        echo -e "  ${CHECK} LakeFormation Admin: ${CALLER_ARN}"
    else
        echo -e "  ${CROSS} LakeFormation Admin: not configured"
    fi

    if check_lakeformation_resource; then
        echo -e "  ${CHECK} LakeFormation Resource: registered"
    else
        echo -e "  ${CROSS} LakeFormation Resource: not registered"
    fi

    if check_glue_catalog; then
        echo -e "  ${CHECK} Glue Catalog: s3tablescatalog"
    else
        echo -e "  ${CROSS} Glue Catalog: s3tablescatalog (not found)"
    fi

    echo ""
    echo "CloudFormation Stack: ${STACK}"
    if check_stack_exists; then
        local status
        status=$(get_stack_status)
        echo -e "  ${CHECK} Status: ${status}"

        local role_arn
        role_arn=$(get_firehose_role_arn)
        if [ -n "$role_arn" ] && [ "$role_arn" != "None" ]; then
            echo ""
            echo "LakeFormation Grants:"
            if check_lakeformation_db_permission "$role_arn"; then
                echo -e "  ${CHECK} DESCRIBE on database '${NAMESPACE}'"
            else
                echo -e "  ${CROSS} DESCRIBE on database '${NAMESPACE}' (not granted)"
            fi

            if check_lakeformation_table_permission "$role_arn" "logs"; then
                echo -e "  ${CHECK} ALL on table 'logs'"
            else
                echo -e "  ${CROSS} ALL on table 'logs' (not granted)"
            fi

            if check_lakeformation_table_permission "$role_arn" "traces"; then
                echo -e "  ${CHECK} ALL on table 'traces'"
            else
                echo -e "  ${CROSS} ALL on table 'traces' (not granted)"
            fi

            if check_lakeformation_table_permission "$role_arn" "sum"; then
                echo -e "  ${CHECK} ALL on table 'sum'"
            else
                echo -e "  ${CROSS} ALL on table 'sum' (not granted)"
            fi

            if check_lakeformation_table_permission "$role_arn" "gauge"; then
                echo -e "  ${CHECK} ALL on table 'gauge'"
            else
                echo -e "  ${CROSS} ALL on table 'gauge' (not granted)"
            fi
        fi

        # Check Firehose streams
        echo ""
        echo "Firehose Streams:"
        local all_streams_ready=true
        for signal in logs traces sum gauge; do
            local stream_name="${STACK}-${signal}"
            local stream_info
            stream_info=$(aws firehose describe-delivery-stream \
                --delivery-stream-name "$stream_name" \
                --region "$REGION" 2>/dev/null)

            if [ -n "$stream_info" ]; then
                # Check if AppendOnly (no UniqueKeys in destination config)
                local has_unique_keys
                has_unique_keys=$(echo "$stream_info" | grep -c "UniqueKeys" || true)
                if [ "$has_unique_keys" = "0" ]; then
                    echo -e "  ${CHECK} ${stream_name} (AppendOnly: true)"
                else
                    echo -e "  ${CHECK} ${stream_name} (AppendOnly: false)"
                fi
            else
                echo -e "  ${CROSS} ${stream_name} (not found)"
                all_streams_ready=false
            fi
        done

        if [ "$all_streams_ready" = true ]; then
            echo ""
            echo -e "${CHECK} Deployment complete! Firehose is ready to receive data."
        else
            echo ""
            echo -e "${ARROW} Some Firehose streams are missing. Run deploy to create them."
        fi
    else
        echo -e "  ${CROSS} Stack does not exist"
    fi
}

# ============================================================================
# Deploy command
# ============================================================================

setup_s3_tables() {
    echo ""
    echo "==> Phase 0: S3 Tables + Lake Formation Setup"

    # Step 1: IAM Role
    echo ""
    echo -e "${ARROW} Creating/updating IAM role: ${ROLE_NAME}"
    if check_iam_role; then
        echo "    Role exists, updating policies..."
        aws iam update-assume-role-policy \
            --role-name "${ROLE_NAME}" \
            --policy-document "${TRUST_POLICY}" >/dev/null
    else
        echo "    Creating role..."
        aws iam create-role \
            --role-name "${ROLE_NAME}" \
            --assume-role-policy-document "${TRUST_POLICY}" \
            --region "${REGION}" >/dev/null
        echo "    Waiting for IAM propagation..."
        sleep 10
    fi
    aws iam put-role-policy \
        --role-name "${ROLE_NAME}" \
        --policy-name "S3TablesDataAccess" \
        --policy-document "${S3_TABLES_POLICY}" >/dev/null
    echo -e "    ${CHECK} Done"

    # Step 2: LakeFormation admin
    echo ""
    echo -e "${ARROW} Adding caller as LakeFormation admin"
    run_idempotent "Failed to set LakeFormation admin" "" \
        aws lakeformation put-data-lake-settings \
        --data-lake-settings "{\"DataLakeAdmins\":[{\"DataLakePrincipalIdentifier\":\"${CALLER_ARN}\"}]}" \
        --region "${REGION}"
    echo -e "    ${CHECK} Done"

    # Step 3: Register resource
    echo ""
    echo -e "${ARROW} Registering S3 Tables resource with LakeFormation"
    local resource_arn="arn:aws:s3tables:${REGION}:${ACCOUNT_ID}:bucket/*"
    local role_arn="arn:aws:iam::${ACCOUNT_ID}:role/${ROLE_NAME}"

    # Deregister first (may not exist, that's fine)
    run_idempotent "Deregister resource" "EntityNotFoundException\|not registered" \
        aws lakeformation deregister-resource \
        --resource-arn "${resource_arn}" \
        --region "${REGION}"

    # Register resource (may already exist with same config)
    run_idempotent "Register resource" "AlreadyExistsException" \
        aws lakeformation register-resource \
        --resource-arn "${resource_arn}" \
        --role-arn "${role_arn}" \
        --with-federation \
        --region "${REGION}"
    echo -e "    ${CHECK} Done"

    # Step 4: Glue catalog
    echo ""
    echo -e "${ARROW} Creating/updating s3tablescatalog federated catalog"
    # Delete first (may not exist, that's fine)
    run_idempotent "Delete catalog" "EntityNotFoundException" \
        aws glue delete-catalog \
        --catalog-id "s3tablescatalog" \
        --region "${REGION}"

    run_idempotent "Create catalog" "AlreadyExistsException" \
        aws glue create-catalog \
        --name "s3tablescatalog" \
        --catalog-input "{
            \"FederatedCatalog\": {
                \"Identifier\": \"${resource_arn}\",
                \"ConnectionName\": \"aws:s3tables\"
            },
            \"CreateDatabaseDefaultPermissions\": [],
            \"CreateTableDefaultPermissions\": [],
            \"CatalogProperties\": {
                \"CustomProperties\": {
                    \"AllowFullTableExternalDataAccess\": \"true\"
                }
            }
        }" \
        --region "${REGION}"
    echo -e "    ${CHECK} Done"

    # Step 5: Catalog permissions
    echo ""
    echo -e "${ARROW} Granting catalog permissions to caller"
    run_idempotent "Grant catalog permissions" "AlreadyExistsException" \
        aws lakeformation grant-permissions \
        --principal "{\"DataLakePrincipalIdentifier\":\"${CALLER_ARN}\"}" \
        --resource "{\"Catalog\":{\"Id\":\"${ACCOUNT_ID}:s3tablescatalog\"}}" \
        --permissions "ALL" "DESCRIBE" "CREATE_DATABASE" "ALTER" "DROP" \
        --permissions-with-grant-option "ALL" "DESCRIBE" "CREATE_DATABASE" "ALTER" "DROP" \
        --region "${REGION}"
    echo -e "    ${CHECK} Done"
}

# ============================================================================
# Firehose stream management via API (for AppendOnly mode)
# ============================================================================

get_stack_output() {
    local output_key="$1"
    aws cloudformation describe-stacks --stack-name "${STACK}" --region "${REGION}" \
        --query "Stacks[0].Outputs[?OutputKey==\`${output_key}\`].OutputValue" \
        --output text 2>/dev/null
}

create_firehose_streams() {
    echo ""
    echo "==> Creating Firehose streams via API (AppendOnly mode)"

    local role_arn
    role_arn=$(get_firehose_role_arn)

    if [ -z "$role_arn" ] || [ "$role_arn" = "None" ]; then
        echo -e "    ${CROSS} Could not find FirehoseRoleARN in stack outputs"
        exit 1
    fi

    # Get configuration from stack outputs
    local bucket_name namespace log_group error_bucket error_prefix batch_time batch_size
    bucket_name=$(get_stack_output "TableBucketName")
    namespace=$(get_stack_output "NamespaceName")
    log_group=$(get_stack_output "FirehoseLogGroupName")
    error_bucket=$(get_stack_output "FirehoseErrorBucketName")
    error_prefix=$(get_stack_output "FirehoseErrorPrefix")
    batch_time=$(get_stack_output "FirehoseBatchTime")
    batch_size=$(get_stack_output "FirehoseBatchSize")

    # Validate required outputs
    [ -z "$bucket_name" ] && { echo -e "    ${CROSS} Missing stack output: TableBucketName"; exit 1; }
    [ -z "$namespace" ] && { echo -e "    ${CROSS} Missing stack output: NamespaceName"; exit 1; }
    [ -z "$log_group" ] && { echo -e "    ${CROSS} Missing stack output: FirehoseLogGroupName"; exit 1; }
    [ -z "$error_bucket" ] && { echo -e "    ${CROSS} Missing stack output: FirehoseErrorBucketName"; exit 1; }
    [ -z "$error_prefix" ] && { echo -e "    ${CROSS} Missing stack output: FirehoseErrorPrefix"; exit 1; }
    [ -z "$batch_time" ] && { echo -e "    ${CROSS} Missing stack output: FirehoseBatchTime"; exit 1; }
    [ -z "$batch_size" ] && { echo -e "    ${CROSS} Missing stack output: FirehoseBatchSize"; exit 1; }

    local catalog_arn="arn:aws:glue:${REGION}:${ACCOUNT_ID}:catalog/s3tablescatalog/${bucket_name}"

    echo "    Role ARN: ${role_arn}"
    echo "    Catalog ARN: ${catalog_arn}"

    local signals=("logs" "traces" "sum" "gauge")
    local log_streams=("Logs_Destination_Errors" "Traces_Destination_Errors" "Sum_Destination_Errors" "Gauge_Destination_Errors")

    for i in "${!signals[@]}"; do
        local signal="${signals[$i]}"
        local log_stream="${log_streams[$i]}"
        local stream_name="${STACK}-${signal}"

        echo ""
        echo -e "${ARROW} Checking stream: ${stream_name}"

        # Check if stream exists
        if aws firehose describe-delivery-stream \
            --delivery-stream-name "$stream_name" \
            --region "$REGION" >/dev/null 2>&1; then
            echo "    Stream exists (skipping)"
            continue
        fi

        echo "    Creating stream with AppendOnly=true..."

        # Build the Iceberg destination configuration
        local iceberg_config
        iceberg_config=$(cat <<EOF
{
    "RoleARN": "${role_arn}",
    "CatalogConfiguration": {
        "CatalogARN": "${catalog_arn}"
    },
    "DestinationTableConfigurationList": [
        {
            "DestinationDatabaseName": "${namespace}",
            "DestinationTableName": "${signal}"
        }
    ],
    "BufferingHints": {
        "IntervalInSeconds": ${batch_time},
        "SizeInMBs": ${batch_size}
    },
    "CloudWatchLoggingOptions": {
        "Enabled": true,
        "LogGroupName": "${log_group}",
        "LogStreamName": "${log_stream}"
    },
    "S3Configuration": {
        "RoleARN": "${role_arn}",
        "BucketARN": "arn:aws:s3:::${error_bucket}",
        "ErrorOutputPrefix": "${error_prefix}${signal}/"
    }
}
EOF
)

        aws firehose create-delivery-stream \
            --delivery-stream-name "$stream_name" \
            --delivery-stream-type DirectPut \
            --iceberg-destination-configuration "$iceberg_config" \
            --region "$REGION" >/dev/null

        echo -e "    ${CHECK} Created"
    done

    echo ""
    echo -e "${CHECK} Firehose streams ready"
}

delete_firehose_streams() {
    echo ""
    echo "==> Deleting Firehose streams"

    local signals=("logs" "traces" "sum" "gauge")

    for signal in "${signals[@]}"; do
        local stream_name="${STACK}-${signal}"

        echo ""
        echo -e "${ARROW} Checking stream: ${stream_name}"

        if aws firehose describe-delivery-stream \
            --delivery-stream-name "$stream_name" \
            --region "$REGION" >/dev/null 2>&1; then
            echo "    Deleting..."
            aws firehose delete-delivery-stream \
                --delivery-stream-name "$stream_name" \
                --region "$REGION" >/dev/null
            echo -e "    ${CHECK} Deleted"
        else
            echo "    Stream does not exist (skipping)"
        fi
    done
}

check_firehose_stream() {
    local stream_name="$1"
    aws firehose describe-delivery-stream \
        --delivery-stream-name "$stream_name" \
        --region "$REGION" 2>/dev/null
}

deploy_cfn() {
    echo ""
    echo "==> Deploying CloudFormation stack"
    echo ""
    echo -e "${ARROW} Deploying stack: ${STACK}"

    aws cloudformation deploy \
        --template-file "${TEMPLATE}" \
        --stack-name "${STACK}" \
        --region "${REGION}" \
        --capabilities CAPABILITY_NAMED_IAM \
        --parameter-overrides "TableBucketName=${BUCKET}" "NamespaceName=${NAMESPACE}" \
        --no-fail-on-empty-changeset

    echo -e "    ${CHECK} CloudFormation complete"
}

grant_lakeformation_permissions() {
    echo ""
    echo "==> Granting LakeFormation permissions to Firehose role"

    local role_arn
    role_arn=$(get_firehose_role_arn)

    if [ -z "$role_arn" ] || [ "$role_arn" = "None" ]; then
        echo -e "    ${CROSS} Could not find FirehoseRoleARN in stack outputs"
        exit 1
    fi

    echo "    Firehose role: ${role_arn}"

    echo ""
    echo -e "${ARROW} Granting DESCRIBE on database '${NAMESPACE}'"
    run_idempotent "Grant database permissions" "AlreadyExistsException" \
        aws lakeformation grant-permissions \
        --region "${REGION}" \
        --principal "{\"DataLakePrincipalIdentifier\":\"${role_arn}\"}" \
        --resource "{\"Database\":{\"CatalogId\":\"${ACCOUNT_ID}:s3tablescatalog/${BUCKET}\",\"Name\":\"${NAMESPACE}\"}}" \
        --permissions DESCRIBE
    echo -e "    ${CHECK} Done"

    echo ""
    echo -e "${ARROW} Granting ALL on table 'logs'"
    run_idempotent "Grant logs table permissions" "AlreadyExistsException" \
        aws lakeformation grant-permissions \
        --region "${REGION}" \
        --principal "{\"DataLakePrincipalIdentifier\":\"${role_arn}\"}" \
        --resource "{\"Table\":{\"CatalogId\":\"${ACCOUNT_ID}:s3tablescatalog/${BUCKET}\",\"DatabaseName\":\"${NAMESPACE}\",\"Name\":\"logs\"}}" \
        --permissions ALL
    echo -e "    ${CHECK} Done"

    echo ""
    echo -e "${ARROW} Granting ALL on table 'traces'"
    run_idempotent "Grant traces table permissions" "AlreadyExistsException" \
        aws lakeformation grant-permissions \
        --region "${REGION}" \
        --principal "{\"DataLakePrincipalIdentifier\":\"${role_arn}\"}" \
        --resource "{\"Table\":{\"CatalogId\":\"${ACCOUNT_ID}:s3tablescatalog/${BUCKET}\",\"DatabaseName\":\"${NAMESPACE}\",\"Name\":\"traces\"}}" \
        --permissions ALL
    echo -e "    ${CHECK} Done"

    echo ""
    echo -e "${ARROW} Granting ALL on table 'sum'"
    run_idempotent "Grant sum table permissions" "AlreadyExistsException" \
        aws lakeformation grant-permissions \
        --region "${REGION}" \
        --principal "{\"DataLakePrincipalIdentifier\":\"${role_arn}\"}" \
        --resource "{\"Table\":{\"CatalogId\":\"${ACCOUNT_ID}:s3tablescatalog/${BUCKET}\",\"DatabaseName\":\"${NAMESPACE}\",\"Name\":\"sum\"}}" \
        --permissions ALL
    echo -e "    ${CHECK} Done"

    echo ""
    echo -e "${ARROW} Granting ALL on table 'gauge'"
    run_idempotent "Grant gauge table permissions" "AlreadyExistsException" \
        aws lakeformation grant-permissions \
        --region "${REGION}" \
        --principal "{\"DataLakePrincipalIdentifier\":\"${role_arn}\"}" \
        --resource "{\"Table\":{\"CatalogId\":\"${ACCOUNT_ID}:s3tablescatalog/${BUCKET}\",\"DatabaseName\":\"${NAMESPACE}\",\"Name\":\"gauge\"}}" \
        --permissions ALL
    echo -e "    ${CHECK} Done"
}

cmd_deploy() {
    if [ -z "$TEMPLATE" ]; then
        echo "Error: Template file is required for deploy"
        echo "Usage: $0 <template.yaml> --region <region> --stack <stack> --bucket <bucket>"
        exit 1
    fi

    if [ ! -f "$TEMPLATE" ]; then
        echo "Error: Template file not found: $TEMPLATE"
        exit 1
    fi

    echo "Deploying otlp2pipeline to AWS"
    echo ""
    get_account_info
    validate_name_lengths
    echo "Account:   ${ACCOUNT_ID}"
    echo "Region:    ${REGION}"
    echo "Stack:     ${STACK}"
    echo "Bucket:    ${BUCKET}"
    echo "Namespace: ${NAMESPACE}"
    echo "Template:  ${TEMPLATE}"

    # S3 Tables setup (always run for idempotency)
    setup_s3_tables

    # Deploy CloudFormation stack (tables, IAM role, logging)
    deploy_cfn

    # Grant LakeFormation permissions
    grant_lakeformation_permissions

    # Create Firehose streams via API (AppendOnly mode)
    create_firehose_streams

    echo ""
    echo "=========================================="
    echo -e "${CHECK} Deployment complete!"
    echo "=========================================="
    echo ""
    echo "Test with:"
    echo "  ./scripts/aws-send-test-record.sh ${STACK} ${REGION}"
    echo ""
    echo "Check status:"
    echo "  $0 status --env ${ENV} --region ${REGION}"
}

# ============================================================================
# Destroy command
# ============================================================================

cmd_destroy() {
    echo "Destroying otlp2pipeline deployment"
    echo ""
    get_account_info
    echo "Account: ${ACCOUNT_ID}"
    echo "Region:  ${REGION}"
    echo "Stack:   ${STACK}"
    echo "Bucket:  ${BUCKET}"
    echo ""

    if [ "$FORCE" != true ]; then
        echo "This will delete:"
        echo "  - Firehose streams: ${STACK}-{logs,traces,sum,gauge}"
        echo "  - CloudFormation stack: ${STACK}"
        echo "  - S3 Table Bucket: ${BUCKET}"
        echo "  - All data in the bucket"
        echo ""
        read -p "Are you sure? (yes/no): " confirm
        if [ "$confirm" != "yes" ]; then
            echo "Aborted."
            exit 0
        fi
    fi

    # Delete Firehose streams first (they depend on IAM role in stack)
    delete_firehose_streams

    # Delete CloudFormation stack
    if check_stack_exists; then
        echo ""
        echo -e "${ARROW} Deleting CloudFormation stack: ${STACK}"
        aws cloudformation delete-stack --stack-name "${STACK}" --region "${REGION}"
        echo "    Waiting for stack deletion..."
        if ! aws cloudformation wait stack-delete-complete --stack-name "${STACK}" --region "${REGION}" 2>&1; then
            local final_status
            final_status=$(get_stack_status)
            echo -e "    ${CROSS} Stack deletion failed. Current status: ${final_status}"
            echo "    Check CloudFormation console for details."
            exit 1
        fi
        echo -e "    ${CHECK} Stack deleted"
    else
        echo ""
        echo "    Stack does not exist (skipping)"
    fi

    echo ""
    echo "=========================================="
    echo -e "${CHECK} Destroy complete"
    echo "=========================================="
    echo ""
    echo "Note: The following global resources were NOT deleted:"
    echo "  - IAM Role: ${ROLE_NAME}"
    echo "  - Glue Catalog: s3tablescatalog"
    echo "  - LakeFormation configuration"
    echo ""
    echo "These are shared across stacks. Delete manually if no longer needed."
}

# ============================================================================
# Main
# ============================================================================

case "${COMMAND}" in
    status)
        cmd_status
        ;;
    destroy)
        cmd_destroy
        ;;
    *)
        cmd_deploy
        ;;
esac
