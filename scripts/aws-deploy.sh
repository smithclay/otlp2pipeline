#!/bin/bash
#
# Unified AWS deployment script for otlp2pipeline
#
# This script handles the complete deployment lifecycle:
# - S3 Tables + Lake Formation setup
# - CloudFormation stack (table bucket, namespace, tables, IAM role, logging)
# - LakeFormation permissions grant
# - Firehose streams via API (AppendOnly mode for unlimited throughput)
#
# Prerequisites:
# - AWS CLI configured with credentials
# - Caller must have IAM and LakeFormation admin permissions
# - CloudFormation template generated by: otlp2pipeline aws create --output template.yaml
#
# Usage:
#   ./scripts/aws-deploy.sh <template> --env <env> [--region <region>] [--namespace <ns>]
#   ./scripts/aws-deploy.sh status --env <env> [--region <region>]
#   ./scripts/aws-deploy.sh destroy --env <env> [--region <region>] [--force]
#
# Examples:
#   ./scripts/aws-deploy.sh template.yaml --env prod
#   ./scripts/aws-deploy.sh template.yaml --env prod --region us-west-2
#   ./scripts/aws-deploy.sh status --env prod
#   ./scripts/aws-deploy.sh destroy --env prod --force
#
# Naming:
#   --env prod  →  stack: otlp2pipeline-prod, bucket: otlp2pipeline-prod
#

set -e

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[0;33m'
NC='\033[0m' # No Color

# Symbols
CHECK="${GREEN}✓${NC}"
CROSS="${RED}✗${NC}"
ARROW="${YELLOW}→${NC}"

# Check prerequisites
if ! command -v aws &> /dev/null; then
    echo -e "${CROSS} AWS CLI is not installed."
    echo "    Install from: https://aws.amazon.com/cli/"
    exit 1
fi

# Defaults
REGION="us-east-1"
NAMESPACE="default"
ROLE_NAME="S3TablesRoleForLakeFormation"
FORCE=false
LOCAL_BUILD=false
ENV=""

# Parse arguments
COMMAND=""
TEMPLATE=""
POSITIONAL=()

while [[ $# -gt 0 ]]; do
    case $1 in
        status|destroy)
            COMMAND="$1"
            shift
            ;;
        --env)
            ENV="$2"
            shift 2
            ;;
        --region)
            REGION="$2"
            shift 2
            ;;
        --namespace)
            NAMESPACE="$2"
            shift 2
            ;;
        --force)
            FORCE=true
            shift
            ;;
        --local)
            LOCAL_BUILD=true
            shift
            ;;
        --help|-h)
            echo "Usage:"
            echo "  $0 <template.yaml> --env <env> [--region <region>] [--namespace <ns>]"
            echo "  $0 status --env <env> [--region <region>]"
            echo "  $0 destroy --env <env> [--region <region>] [--force]"
            echo ""
            echo "Commands:"
            echo "  (default)  Deploy all infrastructure (idempotent)"
            echo "  status     Check current deployment status"
            echo "  destroy    Tear down all infrastructure"
            echo ""
            echo "Options:"
            echo "  --env        Environment name (required) - derives stack and bucket names"
            echo "  --region     AWS region (default: us-east-1)"
            echo "  --namespace  S3 Tables namespace (default: default)"
            echo "  --force      Skip confirmation for destroy"
            echo "  --local      Build and deploy Lambda from local repo (skips GitHub fetch)"
            echo ""
            echo "Naming convention:"
            echo "  --env prod  →  stack: otlp2pipeline-prod, bucket: otlp2pipeline-prod"
            exit 0
            ;;
        -*)
            echo "Unknown option: $1"
            exit 1
            ;;
        *)
            POSITIONAL+=("$1")
            shift
            ;;
    esac
done

# Handle positional arguments
if [ ${#POSITIONAL[@]} -gt 0 ]; then
    if [ -z "$COMMAND" ]; then
        TEMPLATE="${POSITIONAL[0]}"
    fi
fi

# Validate required arguments
if [ -z "$ENV" ]; then
    echo "Error: --env is required"
    exit 1
fi

# Normalize env name (strip otlp2pipeline- prefix if present)
ENV="${ENV#otlp2pipeline-}"

# Derive stack and bucket names from env
STACK="otlp2pipeline-${ENV}"
BUCKET="otlp2pipeline-${ENV}"

# Validate name lengths early (S3 bucket names max 63 chars)
# Error bucket format: ${STACK}-errors-${ACCOUNT_ID}-${REGION}
# With 12-char account ID and up to 14-char region, stack must be ≤28 chars
validate_name_lengths() {
    local stack_len=${#STACK}
    # Error bucket: stack + "-errors-" (8) + account_id (12) + "-" (1) + region (up to 14) = stack + 35
    local max_stack_len=$((63 - 8 - 12 - 1 - ${#REGION}))

    if [ "$stack_len" -gt "$max_stack_len" ]; then
        local error_bucket_len=$((stack_len + 8 + 12 + 1 + ${#REGION}))
        echo -e "${CROSS} Stack name '${STACK}' is too long (${stack_len} chars)"
        echo "    Error bucket would be ${error_bucket_len} chars (max 63)"
        echo "    Max stack name length for region ${REGION}: ${max_stack_len} chars"
        echo ""
        echo "    Use a shorter --env name (current: ${ENV})"
        exit 1
    fi
}

# Get AWS account info
get_account_info() {
    ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text 2>/dev/null) || {
        echo -e "${CROSS} Failed to get AWS account info. Check your credentials."
        exit 1
    }
    CALLER_ARN=$(aws sts get-caller-identity --query Arn --output text 2>/dev/null) || {
        echo -e "${CROSS} Failed to get caller ARN. Check your credentials."
        exit 1
    }
    if [ -z "$CALLER_ARN" ]; then
        echo -e "${CROSS} Caller ARN is empty. Check your AWS credentials."
        exit 1
    fi
}

# Run an idempotent AWS command, accepting specific expected errors
# Usage: run_idempotent "description" "expected_error_pattern" aws command args...
# If expected_error_pattern is empty, any error will cause failure
run_idempotent() {
    local desc="$1"
    local expected_error="$2"
    shift 2

    local output
    if output=$("$@" 2>&1); then
        return 0
    else
        local exit_code=$?
        if [ -n "$expected_error" ] && echo "$output" | grep -q "$expected_error"; then
            # Expected error (e.g., AlreadyExistsException), treat as success
            return 0
        else
            echo -e "    ${CROSS} ${desc}: $output"
            exit $exit_code
        fi
    fi
}

# Wait for Athena query to complete
# Usage: wait_for_athena_query <query_execution_id>
wait_for_athena_query() {
    local query_id="$1"
    local max_attempts=60
    local attempt=0

    while [ $attempt -lt $max_attempts ]; do
        local state
        state=$(aws athena get-query-execution \
            --query-execution-id "$query_id" \
            --region "$REGION" \
            --query 'QueryExecution.Status.State' \
            --output text 2>/dev/null)

        case "$state" in
            SUCCEEDED)
                return 0
                ;;
            FAILED|CANCELLED)
                local reason
                reason=$(aws athena get-query-execution \
                    --query-execution-id "$query_id" \
                    --region "$REGION" \
                    --query 'QueryExecution.Status.StateChangeReason' \
                    --output text 2>/dev/null)
                echo "    Query failed: $reason"
                return 1
                ;;
            *)
                sleep 2
                attempt=$((attempt + 1))
                ;;
        esac
    done

    echo "    Query timed out after $max_attempts attempts"
    return 1
}

# Generate Athena CREATE TABLE DDL from schema JSON
# Usage: generate_athena_ddl <table_name> <bucket> <namespace>
generate_athena_ddl() {
    local table="$1"
    local bucket="$2"
    local namespace="$3"

    # Map table name to schema file (traces -> spans.schema.json)
    local schema_file
    if [ "$table" = "traces" ]; then
        schema_file="schemas/spans.schema.json"
    else
        schema_file="schemas/${table}.schema.json"
    fi

    if [ ! -f "$schema_file" ]; then
        echo "Schema file not found: $schema_file" >&2
        return 1
    fi

    # Transform JSON schema to SQL column definitions
    # Type mapping: int64->bigint, int32->int, float64->double, bool->boolean, json->string
    local columns
    columns=$(jq -r '.fields | map(
        .name + " " + (
            if .type == "int64" then "bigint"
            elif .type == "int32" then "int"
            elif .type == "float64" then "double"
            elif .type == "bool" then "boolean"
            elif .type == "json" then "string"
            else .type end
        )
    ) | join(", ")' "$schema_file")

    cat <<EOF
CREATE TABLE IF NOT EXISTS
  "s3tablescatalog/${bucket}"."${namespace}"."${table}" (
  ${columns}
)
PARTITIONED BY (day(timestamp))
TBLPROPERTIES ('table_type' = 'iceberg')
EOF
}

# Policy documents for S3 Tables role
TRUST_POLICY='{
    "Version": "2012-10-17",
    "Statement": [{
        "Effect": "Allow",
        "Principal": {"Service": "lakeformation.amazonaws.com"},
        "Action": ["sts:AssumeRole", "sts:SetSourceIdentity", "sts:SetContext"]
    }]
}'

S3_TABLES_POLICY='{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "LakeFormationPermissionsForS3ListTableBucket",
            "Effect": "Allow",
            "Action": ["s3tables:ListTableBuckets"],
            "Resource": ["*"]
        },
        {
            "Sid": "LakeFormationDataAccessPermissionsForS3TableBucket",
            "Effect": "Allow",
            "Action": [
                "s3tables:CreateTableBucket",
                "s3tables:GetTableBucket",
                "s3tables:CreateNamespace",
                "s3tables:GetNamespace",
                "s3tables:ListNamespaces",
                "s3tables:DeleteNamespace",
                "s3tables:DeleteTableBucket",
                "s3tables:CreateTable",
                "s3tables:DeleteTable",
                "s3tables:GetTable",
                "s3tables:ListTables",
                "s3tables:RenameTable",
                "s3tables:UpdateTableMetadataLocation",
                "s3tables:GetTableMetadataLocation",
                "s3tables:GetTableData",
                "s3tables:PutTableData"
            ],
            "Resource": ["*"]
        }
    ]
}'

# ============================================================================
# Status checks
# ============================================================================

check_iam_role() {
    aws iam get-role --role-name "${ROLE_NAME}" >/dev/null 2>&1
}

check_lakeformation_admin() {
    local admins
    admins=$(aws lakeformation get-data-lake-settings --region "${REGION}" \
        --query 'DataLakeSettings.DataLakeAdmins[*].DataLakePrincipalIdentifier' \
        --output text 2>/dev/null) || return 1
    echo "$admins" | grep -q "${CALLER_ARN}"
}

check_lakeformation_resource() {
    local resource_arn="arn:aws:s3tables:${REGION}:${ACCOUNT_ID}:bucket/*"
    aws lakeformation describe-resource --resource-arn "${resource_arn}" --region "${REGION}" >/dev/null 2>&1
}

check_glue_catalog() {
    aws glue get-catalog --catalog-id "s3tablescatalog" --region "${REGION}" >/dev/null 2>&1
}

check_stack_exists() {
    aws cloudformation describe-stacks --stack-name "${STACK}" --region "${REGION}" >/dev/null 2>&1
}

get_stack_status() {
    aws cloudformation describe-stacks --stack-name "${STACK}" --region "${REGION}" \
        --query 'Stacks[0].StackStatus' --output text 2>/dev/null
}

get_firehose_role_arn() {
    aws cloudformation describe-stacks --stack-name "${STACK}" --region "${REGION}" \
        --query 'Stacks[0].Outputs[?OutputKey==`FirehoseRoleARN`].OutputValue' \
        --output text 2>/dev/null
}

check_lakeformation_db_permission() {
    local role_arn="$1"
    aws lakeformation list-permissions \
        --principal "{\"DataLakePrincipalIdentifier\":\"${role_arn}\"}" \
        --resource "{\"Database\":{\"CatalogId\":\"${ACCOUNT_ID}:s3tablescatalog/${BUCKET}\",\"Name\":\"${NAMESPACE}\"}}" \
        --region "${REGION}" \
        --query 'PrincipalResourcePermissions[0]' \
        --output text 2>/dev/null | grep -q DESCRIBE
}

check_lakeformation_table_permission() {
    local role_arn="$1"
    local table_name="$2"
    aws lakeformation list-permissions \
        --principal "{\"DataLakePrincipalIdentifier\":\"${role_arn}\"}" \
        --resource "{\"Table\":{\"CatalogId\":\"${ACCOUNT_ID}:s3tablescatalog/${BUCKET}\",\"DatabaseName\":\"${NAMESPACE}\",\"Name\":\"${table_name}\"}}" \
        --region "${REGION}" \
        --query 'PrincipalResourcePermissions[0]' \
        --output text 2>/dev/null | grep -q ALL
}

check_partition_spec() {
    # Partition evolution is currently disabled - always return false
    # See docs/aws-partition-evolution.md
    return 1
}

# ============================================================================
# Status command
# ============================================================================

cmd_status() {
    echo "Checking deployment status..."
    echo ""
    get_account_info
    echo "Account: ${ACCOUNT_ID}"
    echo "Region:  ${REGION}"
    echo "Stack:   ${STACK}"
    echo "Bucket:  ${BUCKET}"
    echo ""

    echo "S3 Tables Setup:"
    if check_iam_role; then
        echo -e "  ${CHECK} IAM Role: ${ROLE_NAME}"
    else
        echo -e "  ${CROSS} IAM Role: ${ROLE_NAME} (not found)"
    fi

    if check_lakeformation_admin; then
        echo -e "  ${CHECK} LakeFormation Admin: ${CALLER_ARN}"
    else
        echo -e "  ${CROSS} LakeFormation Admin: not configured"
    fi

    if check_lakeformation_resource; then
        echo -e "  ${CHECK} LakeFormation Resource: registered"
    else
        echo -e "  ${CROSS} LakeFormation Resource: not registered"
    fi

    if check_glue_catalog; then
        echo -e "  ${CHECK} Glue Catalog: s3tablescatalog"
    else
        echo -e "  ${CROSS} Glue Catalog: s3tablescatalog (not found)"
    fi

    echo ""
    echo "CloudFormation Stack: ${STACK}"
    if check_stack_exists; then
        local status
        status=$(get_stack_status)
        echo -e "  ${CHECK} Status: ${status}"

        local role_arn
        role_arn=$(get_firehose_role_arn)
        if [ -n "$role_arn" ] && [ "$role_arn" != "None" ]; then
            echo ""
            echo "LakeFormation Grants:"
            if check_lakeformation_db_permission "$role_arn"; then
                echo -e "  ${CHECK} DESCRIBE on database '${NAMESPACE}'"
            else
                echo -e "  ${CROSS} DESCRIBE on database '${NAMESPACE}' (not granted)"
            fi

            if check_lakeformation_table_permission "$role_arn" "logs"; then
                echo -e "  ${CHECK} ALL on table 'logs'"
            else
                echo -e "  ${CROSS} ALL on table 'logs' (not granted)"
            fi

            if check_lakeformation_table_permission "$role_arn" "traces"; then
                echo -e "  ${CHECK} ALL on table 'traces'"
            else
                echo -e "  ${CROSS} ALL on table 'traces' (not granted)"
            fi

            if check_lakeformation_table_permission "$role_arn" "sum"; then
                echo -e "  ${CHECK} ALL on table 'sum'"
            else
                echo -e "  ${CROSS} ALL on table 'sum' (not granted)"
            fi

            if check_lakeformation_table_permission "$role_arn" "gauge"; then
                echo -e "  ${CHECK} ALL on table 'gauge'"
            else
                echo -e "  ${CROSS} ALL on table 'gauge' (not granted)"
            fi
        fi

        # Check if tables have partition specs
        echo ""
        echo "Table Partitions:"
        for table in logs traces sum gauge; do
            local table_info
            table_info=$(aws glue get-table \
                --catalog-id "${ACCOUNT_ID}:s3tablescatalog/${BUCKET}" \
                --database-name "${NAMESPACE}" \
                --name "$table" \
                --region "$REGION" 2>/dev/null)

            if [ -n "$table_info" ]; then
                local partition_keys
                partition_keys=$(echo "$table_info" | jq -r '.Table.PartitionKeys // [] | length')
                if [ "$partition_keys" != "0" ]; then
                    echo -e "  ${CHECK} ${table}: partitioned"
                else
                    echo -e "  ${CROSS} ${table}: not partitioned"
                fi
            else
                echo -e "  ${CROSS} ${table}: not found"
            fi
        done

        # Check Firehose streams
        echo ""
        echo "Firehose Streams:"
        local all_streams_ready=true
        for signal in logs traces sum gauge; do
            local stream_name="${STACK}-${signal}"
            local stream_info
            stream_info=$(aws firehose describe-delivery-stream \
                --delivery-stream-name "$stream_name" \
                --region "$REGION" 2>/dev/null)

            if [ -n "$stream_info" ]; then
                # Check if AppendOnly (no UniqueKeys in destination config)
                local has_unique_keys
                has_unique_keys=$(echo "$stream_info" | grep -c "UniqueKeys" || true)
                if [ "$has_unique_keys" = "0" ]; then
                    echo -e "  ${CHECK} ${stream_name} (AppendOnly: true)"
                else
                    echo -e "  ${CHECK} ${stream_name} (AppendOnly: false)"
                fi
            else
                echo -e "  ${CROSS} ${stream_name} (not found)"
                all_streams_ready=false
            fi
        done

        if [ "$all_streams_ready" = true ]; then
            echo ""
            echo -e "${CHECK} Deployment complete! Firehose is ready to receive data."
        else
            echo ""
            echo -e "${ARROW} Some Firehose streams are missing. Run deploy to create them."
        fi
    else
        echo -e "  ${CROSS} Stack does not exist"
    fi
}

# ============================================================================
# Deploy command
# ============================================================================

setup_s3_tables() {
    echo ""
    echo "==> Phase 0: S3 Tables + Lake Formation Setup"

    # Step 1: IAM Role
    echo ""
    echo -e "${ARROW} Creating/updating IAM role: ${ROLE_NAME}"
    if check_iam_role; then
        echo "    Role exists, updating policies..."
        aws iam update-assume-role-policy \
            --role-name "${ROLE_NAME}" \
            --policy-document "${TRUST_POLICY}" >/dev/null
    else
        echo "    Creating role..."
        aws iam create-role \
            --role-name "${ROLE_NAME}" \
            --assume-role-policy-document "${TRUST_POLICY}" \
            --region "${REGION}" >/dev/null
        echo "    Waiting for IAM propagation..."
        sleep 10
    fi
    aws iam put-role-policy \
        --role-name "${ROLE_NAME}" \
        --policy-name "S3TablesDataAccess" \
        --policy-document "${S3_TABLES_POLICY}" >/dev/null
    echo -e "    ${CHECK} Done"

    # Step 2: LakeFormation admin
    echo ""
    echo -e "${ARROW} Adding caller as LakeFormation admin"
    run_idempotent "Failed to set LakeFormation admin" "" \
        aws lakeformation put-data-lake-settings \
        --data-lake-settings "{\"DataLakeAdmins\":[{\"DataLakePrincipalIdentifier\":\"${CALLER_ARN}\"}]}" \
        --region "${REGION}"
    echo -e "    ${CHECK} Done"

    # Step 3: Register resource
    echo ""
    echo -e "${ARROW} Registering S3 Tables resource with LakeFormation"
    local resource_arn="arn:aws:s3tables:${REGION}:${ACCOUNT_ID}:bucket/*"
    local role_arn="arn:aws:iam::${ACCOUNT_ID}:role/${ROLE_NAME}"

    # Deregister first (may not exist, that's fine)
    run_idempotent "Deregister resource" "EntityNotFoundException\|not registered" \
        aws lakeformation deregister-resource \
        --resource-arn "${resource_arn}" \
        --region "${REGION}"

    # Register resource (may already exist with same config)
    run_idempotent "Register resource" "AlreadyExistsException" \
        aws lakeformation register-resource \
        --resource-arn "${resource_arn}" \
        --role-arn "${role_arn}" \
        --with-federation \
        --region "${REGION}"
    echo -e "    ${CHECK} Done"

    # Step 4: Glue catalog
    echo ""
    echo -e "${ARROW} Creating/updating s3tablescatalog federated catalog"
    # Delete first (may not exist, that's fine)
    run_idempotent "Delete catalog" "EntityNotFoundException" \
        aws glue delete-catalog \
        --catalog-id "s3tablescatalog" \
        --region "${REGION}"

    run_idempotent "Create catalog" "AlreadyExistsException" \
        aws glue create-catalog \
        --name "s3tablescatalog" \
        --catalog-input "{
            \"FederatedCatalog\": {
                \"Identifier\": \"${resource_arn}\",
                \"ConnectionName\": \"aws:s3tables\"
            },
            \"CreateDatabaseDefaultPermissions\": [],
            \"CreateTableDefaultPermissions\": [],
            \"CatalogProperties\": {
                \"CustomProperties\": {
                    \"AllowFullTableExternalDataAccess\": \"true\"
                }
            }
        }" \
        --region "${REGION}"
    echo -e "    ${CHECK} Done"

    # Step 5: Catalog permissions
    echo ""
    echo -e "${ARROW} Granting catalog permissions to caller"
    run_idempotent "Grant catalog permissions" "AlreadyExistsException" \
        aws lakeformation grant-permissions \
        --principal "{\"DataLakePrincipalIdentifier\":\"${CALLER_ARN}\"}" \
        --resource "{\"Catalog\":{\"Id\":\"${ACCOUNT_ID}:s3tablescatalog\"}}" \
        --permissions "ALL" "DESCRIBE" "CREATE_DATABASE" "ALTER" "DROP" \
        --permissions-with-grant-option "ALL" "DESCRIBE" "CREATE_DATABASE" "ALTER" "DROP" \
        --region "${REGION}"
    echo -e "    ${CHECK} Done"
}

# ============================================================================
# Firehose stream management via API (for AppendOnly mode)
# ============================================================================

get_stack_output() {
    local output_key="$1"
    aws cloudformation describe-stacks --stack-name "${STACK}" --region "${REGION}" \
        --query "Stacks[0].Outputs[?OutputKey==\`${output_key}\`].OutputValue" \
        --output text 2>/dev/null
}

create_firehose_streams() {
    echo ""
    echo "==> Creating Firehose streams via API (AppendOnly mode)"

    local role_arn
    role_arn=$(get_firehose_role_arn)

    if [ -z "$role_arn" ] || [ "$role_arn" = "None" ]; then
        echo -e "    ${CROSS} Could not find FirehoseRoleARN in stack outputs"
        exit 1
    fi

    # Get configuration from stack outputs
    local bucket_name namespace log_group error_bucket error_prefix batch_time batch_size
    bucket_name=$(get_stack_output "TableBucketName")
    namespace=$(get_stack_output "NamespaceName")
    log_group=$(get_stack_output "FirehoseLogGroupName")
    error_bucket=$(get_stack_output "FirehoseErrorBucketName")
    error_prefix=$(get_stack_output "FirehoseErrorPrefix")
    batch_time=$(get_stack_output "FirehoseBatchTime")
    batch_size=$(get_stack_output "FirehoseBatchSize")

    # Validate required outputs
    [ -z "$bucket_name" ] && { echo -e "    ${CROSS} Missing stack output: TableBucketName"; exit 1; }
    [ -z "$namespace" ] && { echo -e "    ${CROSS} Missing stack output: NamespaceName"; exit 1; }
    [ -z "$log_group" ] && { echo -e "    ${CROSS} Missing stack output: FirehoseLogGroupName"; exit 1; }
    [ -z "$error_bucket" ] && { echo -e "    ${CROSS} Missing stack output: FirehoseErrorBucketName"; exit 1; }
    [ -z "$error_prefix" ] && { echo -e "    ${CROSS} Missing stack output: FirehoseErrorPrefix"; exit 1; }
    [ -z "$batch_time" ] && { echo -e "    ${CROSS} Missing stack output: FirehoseBatchTime"; exit 1; }
    [ -z "$batch_size" ] && { echo -e "    ${CROSS} Missing stack output: FirehoseBatchSize"; exit 1; }

    local catalog_arn="arn:aws:glue:${REGION}:${ACCOUNT_ID}:catalog/s3tablescatalog/${bucket_name}"

    echo "    Role ARN: ${role_arn}"
    echo "    Catalog ARN: ${catalog_arn}"

    local signals=("logs" "traces" "sum" "gauge")
    local log_streams=("Logs_Destination_Errors" "Traces_Destination_Errors" "Sum_Destination_Errors" "Gauge_Destination_Errors")

    for i in "${!signals[@]}"; do
        local signal="${signals[$i]}"
        local log_stream="${log_streams[$i]}"
        local stream_name="${STACK}-${signal}"

        echo ""
        echo -e "${ARROW} Checking stream: ${stream_name}"

        # Check if stream exists
        if aws firehose describe-delivery-stream \
            --delivery-stream-name "$stream_name" \
            --region "$REGION" >/dev/null 2>&1; then
            echo "    Stream exists (skipping)"
            continue
        fi

        echo "    Creating stream with AppendOnly=true..."

        # Build the Iceberg destination configuration
        local iceberg_config
        iceberg_config=$(cat <<EOF
{
    "RoleARN": "${role_arn}",
    "CatalogConfiguration": {
        "CatalogARN": "${catalog_arn}"
    },
    "DestinationTableConfigurationList": [
        {
            "DestinationDatabaseName": "${namespace}",
            "DestinationTableName": "${signal}"
        }
    ],
    "BufferingHints": {
        "IntervalInSeconds": ${batch_time},
        "SizeInMBs": ${batch_size}
    },
    "CloudWatchLoggingOptions": {
        "Enabled": true,
        "LogGroupName": "${log_group}",
        "LogStreamName": "${log_stream}"
    },
    "S3Configuration": {
        "RoleARN": "${role_arn}",
        "BucketARN": "arn:aws:s3:::${error_bucket}",
        "ErrorOutputPrefix": "${error_prefix}${signal}/"
    }
}
EOF
)

        aws firehose create-delivery-stream \
            --delivery-stream-name "$stream_name" \
            --delivery-stream-type DirectPut \
            --iceberg-destination-configuration "$iceberg_config" \
            --region "$REGION" >/dev/null

        echo -e "    ${CHECK} Created"
    done

    echo ""
    echo -e "${CHECK} Firehose streams ready"
}

delete_firehose_streams() {
    echo ""
    echo "==> Deleting Firehose streams"

    local signals=("logs" "traces" "sum" "gauge")

    for signal in "${signals[@]}"; do
        local stream_name="${STACK}-${signal}"

        echo ""
        echo -e "${ARROW} Checking stream: ${stream_name}"

        if aws firehose describe-delivery-stream \
            --delivery-stream-name "$stream_name" \
            --region "$REGION" >/dev/null 2>&1; then
            echo "    Deleting..."
            aws firehose delete-delivery-stream \
                --delivery-stream-name "$stream_name" \
                --region "$REGION" >/dev/null
            echo -e "    ${CHECK} Deleted"
        else
            echo "    Stream does not exist (skipping)"
        fi
    done
}

check_firehose_stream() {
    local stream_name="$1"
    aws firehose describe-delivery-stream \
        --delivery-stream-name "$stream_name" \
        --region "$REGION" 2>/dev/null
}

deploy_cfn() {
    echo ""
    echo "==> Deploying CloudFormation stack"
    echo ""
    echo -e "${ARROW} Deploying stack: ${STACK}"

    local params="TableBucketName=${BUCKET} NamespaceName=${NAMESPACE}"
    if [ "$LOCAL_BUILD" = true ]; then
        params="$params SkipLambda=true"
        echo "    (Lambda will be deployed separately from local build)"
    fi

    aws cloudformation deploy \
        --template-file "${TEMPLATE}" \
        --stack-name "${STACK}" \
        --region "${REGION}" \
        --capabilities CAPABILITY_NAMED_IAM \
        --parameter-overrides $params \
        --no-fail-on-empty-changeset

    echo -e "    ${CHECK} CloudFormation complete"
}

grant_lakeformation_permissions() {
    echo ""
    echo "==> Granting LakeFormation permissions to Firehose role"

    local role_arn
    role_arn=$(get_firehose_role_arn)

    if [ -z "$role_arn" ] || [ "$role_arn" = "None" ]; then
        echo -e "    ${CROSS} Could not find FirehoseRoleARN in stack outputs"
        exit 1
    fi

    echo "    Firehose role: ${role_arn}"

    echo ""
    echo -e "${ARROW} Granting DESCRIBE on database '${NAMESPACE}'"
    run_idempotent "Grant database permissions" "AlreadyExistsException" \
        aws lakeformation grant-permissions \
        --region "${REGION}" \
        --principal "{\"DataLakePrincipalIdentifier\":\"${role_arn}\"}" \
        --resource "{\"Database\":{\"CatalogId\":\"${ACCOUNT_ID}:s3tablescatalog/${BUCKET}\",\"Name\":\"${NAMESPACE}\"}}" \
        --permissions DESCRIBE
    echo -e "    ${CHECK} Done"

    echo ""
    echo -e "${ARROW} Granting ALL on table 'logs'"
    run_idempotent "Grant logs table permissions" "AlreadyExistsException" \
        aws lakeformation grant-permissions \
        --region "${REGION}" \
        --principal "{\"DataLakePrincipalIdentifier\":\"${role_arn}\"}" \
        --resource "{\"Table\":{\"CatalogId\":\"${ACCOUNT_ID}:s3tablescatalog/${BUCKET}\",\"DatabaseName\":\"${NAMESPACE}\",\"Name\":\"logs\"}}" \
        --permissions ALL
    echo -e "    ${CHECK} Done"

    echo ""
    echo -e "${ARROW} Granting ALL on table 'traces'"
    run_idempotent "Grant traces table permissions" "AlreadyExistsException" \
        aws lakeformation grant-permissions \
        --region "${REGION}" \
        --principal "{\"DataLakePrincipalIdentifier\":\"${role_arn}\"}" \
        --resource "{\"Table\":{\"CatalogId\":\"${ACCOUNT_ID}:s3tablescatalog/${BUCKET}\",\"DatabaseName\":\"${NAMESPACE}\",\"Name\":\"traces\"}}" \
        --permissions ALL
    echo -e "    ${CHECK} Done"

    echo ""
    echo -e "${ARROW} Granting ALL on table 'sum'"
    run_idempotent "Grant sum table permissions" "AlreadyExistsException" \
        aws lakeformation grant-permissions \
        --region "${REGION}" \
        --principal "{\"DataLakePrincipalIdentifier\":\"${role_arn}\"}" \
        --resource "{\"Table\":{\"CatalogId\":\"${ACCOUNT_ID}:s3tablescatalog/${BUCKET}\",\"DatabaseName\":\"${NAMESPACE}\",\"Name\":\"sum\"}}" \
        --permissions ALL
    echo -e "    ${CHECK} Done"

    echo ""
    echo -e "${ARROW} Granting ALL on table 'gauge'"
    run_idempotent "Grant gauge table permissions" "AlreadyExistsException" \
        aws lakeformation grant-permissions \
        --region "${REGION}" \
        --principal "{\"DataLakePrincipalIdentifier\":\"${role_arn}\"}" \
        --resource "{\"Table\":{\"CatalogId\":\"${ACCOUNT_ID}:s3tablescatalog/${BUCKET}\",\"DatabaseName\":\"${NAMESPACE}\",\"Name\":\"gauge\"}}" \
        --permissions ALL
    echo -e "    ${CHECK} Done"
}

# ============================================================================
# Create tables via Athena DDL (with partition specs)
# ============================================================================
#
# Tables are created via Athena DDL instead of CloudFormation because:
# 1. AWS::S3Tables::Table does not support PartitionSpec
# 2. Athena CREATE TABLE supports PARTITIONED BY (day(timestamp))
#
# See docs/aws-partition-evolution.md for background.
#

create_tables_via_athena() {
    echo ""
    echo "==> Creating tables via Athena DDL (with partitions)"

    local bucket_name namespace error_bucket
    bucket_name=$(get_stack_output "TableBucketName")
    namespace=$(get_stack_output "NamespaceName")
    error_bucket=$(get_stack_output "FirehoseErrorBucketName")

    if [ -z "$bucket_name" ] || [ "$bucket_name" = "None" ]; then
        echo -e "    ${CROSS} Missing stack output: TableBucketName"
        return 1
    fi

    local tables=("logs" "traces" "sum" "gauge")

    for table in "${tables[@]}"; do
        echo ""
        echo -e "${ARROW} Creating table: ${table}"

        # Generate DDL
        local ddl
        ddl=$(generate_athena_ddl "$table" "$bucket_name" "$namespace")

        if [ $? -ne 0 ]; then
            echo -e "    ${CROSS} Failed to generate DDL"
            return 1
        fi

        # Execute via Athena
        local query_id
        query_id=$(aws athena start-query-execution \
            --query-string "$ddl" \
            --query-execution-context "Catalog=s3tablescatalog" \
            --result-configuration "OutputLocation=s3://${error_bucket}/athena/" \
            --region "$REGION" \
            --output text \
            --query 'QueryExecutionId' 2>&1)

        if [ $? -ne 0 ]; then
            echo -e "    ${CROSS} Failed to start query: $query_id"
            return 1
        fi

        echo "    Query ID: $query_id"

        # Wait for completion
        if wait_for_athena_query "$query_id"; then
            echo -e "    ${CHECK} Created with day(timestamp) partition"
        else
            echo -e "    ${CROSS} Query failed"
            return 1
        fi
    done

    echo ""
    echo -e "${CHECK} All tables created with partitions"
}

# ============================================================================
# Local Lambda build and deploy
# ============================================================================

build_and_deploy_lambda() {
    echo ""
    echo "==> Building and deploying Lambda from local repo"

    # Check for cargo-lambda
    if ! command -v cargo-lambda &> /dev/null; then
        echo -e "    ${CROSS} cargo-lambda not found. Install with: pip3 install cargo-lambda"
        exit 1
    fi

    # Build Lambda
    echo ""
    echo -e "${ARROW} Building Lambda (ARM64)"
    cargo lambda build --release --arm64 --features lambda --bin lambda 2>&1 | tail -5

    if [ $? -ne 0 ]; then
        echo -e "    ${CROSS} Build failed"
        exit 1
    fi
    echo -e "    ${CHECK} Build complete"

    # Get artifact bucket from stack outputs
    local artifact_bucket
    artifact_bucket=$(get_stack_output "ArtifactBucketName" 2>/dev/null)

    # If no output, construct the bucket name
    if [ -z "$artifact_bucket" ] || [ "$artifact_bucket" = "None" ]; then
        artifact_bucket="${STACK}-artifacts-${ACCOUNT_ID}"
    fi

    # Zip the bootstrap binary
    echo ""
    echo -e "${ARROW} Uploading to S3"
    local build_dir="target/lambda/lambda"
    local zip_path="/tmp/lambda-${STACK}.zip"

    (cd "$build_dir" && zip -j "$zip_path" bootstrap) >/dev/null
    local s3_key="lambda/local/bootstrap.zip"

    aws s3 cp "$zip_path" "s3://${artifact_bucket}/${s3_key}" --region "${REGION}" >/dev/null
    echo -e "    ${CHECK} Uploaded to s3://${artifact_bucket}/${s3_key}"

    # Create or update Lambda function
    echo ""
    echo -e "${ARROW} Creating/updating Lambda function"

    local function_name="${STACK}-ingest"
    local role_arn="arn:aws:iam::${ACCOUNT_ID}:role/${STACK}-Lambda-${REGION}"

    # Check if function exists
    if aws lambda get-function --function-name "$function_name" --region "$REGION" >/dev/null 2>&1; then
        # Update existing function
        aws lambda update-function-code \
            --function-name "$function_name" \
            --s3-bucket "$artifact_bucket" \
            --s3-key "$s3_key" \
            --region "$REGION" >/dev/null

        echo -e "    ${CHECK} Updated function: ${function_name}"
    else
        # Create new function
        aws lambda create-function \
            --function-name "$function_name" \
            --runtime provided.al2023 \
            --architectures arm64 \
            --handler bootstrap \
            --role "$role_arn" \
            --memory-size 512 \
            --timeout 30 \
            --code "S3Bucket=${artifact_bucket},S3Key=${s3_key}" \
            --environment "Variables={RUST_LOG=info,PIPELINE_LOGS=${STACK}-logs,PIPELINE_TRACES=${STACK}-traces,PIPELINE_SUM=${STACK}-sum,PIPELINE_GAUGE=${STACK}-gauge}" \
            --region "$REGION" >/dev/null

        echo -e "    ${CHECK} Created function: ${function_name}"

        # Create function URL
        echo ""
        echo -e "${ARROW} Creating function URL"
        aws lambda create-function-url-config \
            --function-name "$function_name" \
            --auth-type NONE \
            --region "$REGION" >/dev/null 2>&1 || true

        aws lambda add-permission \
            --function-name "$function_name" \
            --statement-id FunctionURLAllowPublicAccess \
            --action lambda:InvokeFunctionUrl \
            --principal "*" \
            --function-url-auth-type NONE \
            --region "$REGION" >/dev/null 2>&1 || true

        echo -e "    ${CHECK} Function URL created"
    fi

    # Get and display function URL
    local function_url
    function_url=$(aws lambda get-function-url-config \
        --function-name "$function_name" \
        --region "$REGION" \
        --query 'FunctionUrl' \
        --output text 2>/dev/null || echo "")

    if [ -n "$function_url" ] && [ "$function_url" != "None" ]; then
        echo ""
        echo "    Function URL: ${function_url}"
    fi

    rm -f "$zip_path"
    echo ""
    echo -e "${CHECK} Lambda deployed from local build"
}

cmd_deploy() {
    if [ -z "$TEMPLATE" ]; then
        echo "Error: Template file is required for deploy"
        echo "Usage: $0 <template.yaml> --region <region> --stack <stack> --bucket <bucket>"
        exit 1
    fi

    if [ ! -f "$TEMPLATE" ]; then
        echo "Error: Template file not found: $TEMPLATE"
        exit 1
    fi

    echo "Deploying otlp2pipeline to AWS"
    echo ""
    get_account_info
    validate_name_lengths
    echo "Account:   ${ACCOUNT_ID}"
    echo "Region:    ${REGION}"
    echo "Stack:     ${STACK}"
    echo "Bucket:    ${BUCKET}"
    echo "Namespace: ${NAMESPACE}"
    echo "Template:  ${TEMPLATE}"

    # S3 Tables setup (always run for idempotency)
    setup_s3_tables

    # Deploy CloudFormation stack (bucket, namespace, IAM role, logging)
    deploy_cfn

    # Create tables via Athena DDL (with partition specs)
    create_tables_via_athena

    # Grant LakeFormation permissions (after tables exist)
    grant_lakeformation_permissions

    # Create Firehose streams via API (AppendOnly mode)
    create_firehose_streams

    # Build and deploy Lambda from local repo (if --local flag)
    if [ "$LOCAL_BUILD" = true ]; then
        build_and_deploy_lambda
    fi

    echo ""
    echo "=========================================="
    echo -e "${CHECK} Deployment complete!"
    echo "=========================================="
    echo ""
    echo "Test with:"
    echo "  ./scripts/aws-send-test-record.sh ${STACK} ${REGION}"
    echo ""
    echo "Check status:"
    echo "  $0 status --env ${ENV} --region ${REGION}"
}

# ============================================================================
# Destroy command
# ============================================================================

cmd_destroy() {
    echo "Destroying otlp2pipeline deployment"
    echo ""
    get_account_info
    echo "Account: ${ACCOUNT_ID}"
    echo "Region:  ${REGION}"
    echo "Stack:   ${STACK}"
    echo "Bucket:  ${BUCKET}"
    echo ""

    if [ "$FORCE" != true ]; then
        echo "This will delete:"
        echo "  - Firehose streams: ${STACK}-{logs,traces,sum,gauge}"
        echo "  - CloudFormation stack: ${STACK}"
        echo "  - S3 Table Bucket: ${BUCKET}"
        echo "  - All data in the bucket"
        echo ""
        read -p "Are you sure? (yes/no): " confirm
        if [ "$confirm" != "yes" ]; then
            echo "Aborted."
            exit 0
        fi
    fi

    # Delete Firehose streams first (they depend on IAM role in stack)
    delete_firehose_streams

    # Delete CloudFormation stack
    if check_stack_exists; then
        echo ""
        echo -e "${ARROW} Deleting CloudFormation stack: ${STACK}"
        aws cloudformation delete-stack --stack-name "${STACK}" --region "${REGION}"
        echo "    Waiting for stack deletion..."
        if ! aws cloudformation wait stack-delete-complete --stack-name "${STACK}" --region "${REGION}" 2>&1; then
            local final_status
            final_status=$(get_stack_status)
            echo -e "    ${CROSS} Stack deletion failed. Current status: ${final_status}"
            echo "    Check CloudFormation console for details."
            exit 1
        fi
        echo -e "    ${CHECK} Stack deleted"
    else
        echo ""
        echo "    Stack does not exist (skipping)"
    fi

    echo ""
    echo "=========================================="
    echo -e "${CHECK} Destroy complete"
    echo "=========================================="
    echo ""
    echo "Note: The following global resources were NOT deleted:"
    echo "  - IAM Role: ${ROLE_NAME}"
    echo "  - Glue Catalog: s3tablescatalog"
    echo "  - LakeFormation configuration"
    echo ""
    echo "These are shared across stacks. Delete manually if no longer needed."
}

# ============================================================================
# Main
# ============================================================================

case "${COMMAND}" in
    status)
        cmd_status
        ;;
    destroy)
        cmd_destroy
        ;;
    *)
        cmd_deploy
        ;;
esac
